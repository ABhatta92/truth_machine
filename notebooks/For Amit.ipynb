{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee466982",
   "metadata": {},
   "source": [
    "## Ingesting raw UNODC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ea6f6-df2c-476f-8034-c58ed9082c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # Collecting payloads from URLs\n",
    "import os # Great for navigating inside your file system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining root and raw for ingestion\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "data_root = os.path.join(project_root, 'data')\n",
    "raw_path = os.path.join(data_root, 'raw')\n",
    "print(data_root)\n",
    "print(raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a914df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have a complete data dump, we can overwrite it every time\n",
    "def get_file_data(url):\n",
    "    file_name = url.split('/')[-1] # Gets file name\n",
    "    folder_name = os.path.join(raw_path, file_name.split('.')[0]) # Removing the file extension for folder name\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Creating and writing the files to directory\n",
    "    os.makedirs(folder_name, exist_ok=True) \n",
    "    file_path = os.path.join(folder_name, file_name) \n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f'extracted file to {folder_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da87f0-9fd7-4a73-bb4b-2c9ef6c66ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3 Files from UNODC based on their classification\n",
    "file_urls = ['https://dataunodc.un.org/sites/dataunodc.un.org/files/data_cts_violent_and_sexual_crime.xlsx',\n",
    "             'https://dataunodc.un.org/sites/dataunodc.un.org/files/data_cts_access_and_functioning_of_justice.xlsx',\n",
    "             'https://dataunodc.un.org/sites/dataunodc.un.org/files/data_cts_corruption_and_economic_crime.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062db81-f6c2-4b12-8172-5132c2eba311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go\n",
    "for url in file_urls:\n",
    "        get_file_data(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a68cb",
   "metadata": {},
   "source": [
    "## Raw to bronze processing\n",
    "- Info.txt has the nature of raw to bronze transformations\n",
    "- We'll go through it using spark and pandas side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6ffbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# from utils.common_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847f1429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need a SparkSession\n",
    "# This is the entrypoint of any spark application\n",
    "\n",
    "spark = SparkSession.builder.appName(\"raw_to_bronze_unodc\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1e3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.yml has all the paths that we need\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "    \n",
    "config_path = r'D:\\python_projects\\truth_machine\\config.yaml'.replace(\"\\\\\", \"/\")\n",
    "config = load_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea41457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is the best for reading excel since it deals with weird formatting better\n",
    "# Spark isn't very good with varying schema because it's meant for huge data\n",
    "# It will be spark.read.excel(file_path) in pyspark\n",
    "df_func_justice = pd.read_excel(config['raw_paths']['unodc_func_justice'], engine='openpyxl', header=2)\n",
    "df_econ_crime = pd.read_excel(config['raw_paths']['unodc_econ_crime'], engine='openpyxl', header=2)\n",
    "df_violent_sexual = pd.read_excel(config['raw_paths']['unodc_sexual_violent'], engine='openpyxl', header=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7d6fed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Converting between spark and pandas is easy as balls\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_func_justice_spark = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_func_justice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m df_econ_crime_spark = spark.createDataFrame(df_econ_crime)\n\u001b[32m      4\u001b[39m df_violent_sexual_spark = spark.createDataFrame(df_violent_sexual)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python_projects\\truth_machine\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:1273\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1269\u001b[39m     data = pd.DataFrame(data, columns=column_names)\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd.DataFrame):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[32m   1274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_dataframe(\n\u001b[32m   1277\u001b[39m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1278\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python_projects\\truth_machine\\.venv\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:407\u001b[39m, in \u001b[36mSparkConversionMixin.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, SparkSession)\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m timezone = \u001b[38;5;28mself\u001b[39m._jconf.sessionLocalTimeZone()\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m# If no schema supplied by user then get the names of columns only\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python_projects\\truth_machine\\.venv\\Lib\\site-packages\\pyspark\\sql\\pandas\\utils.py:24\u001b[39m, in \u001b[36mrequire_minimum_pandas_version\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# TODO(HyukjinKwon): Relocate and deduplicate the version specification.\u001b[39;00m\n\u001b[32m     22\u001b[39m minimum_pandas_version = \u001b[33m\"\u001b[39m\u001b[33m1.0.5\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdistutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "# Converting between spark and pandas is easy as balls\n",
    "df_func_justice_spark = spark.createDataFrame(df_func_justice)\n",
    "df_econ_crime_spark = spark.createDataFrame(df_econ_crime)\n",
    "df_violent_sexual_spark = spark.createDataFrame(df_violent_sexual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673f1ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
